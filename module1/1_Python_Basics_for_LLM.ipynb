{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b304a6c",
   "metadata": {},
   "source": [
    "**Coursebook: Python Programming Basics for Large Language Models (LLMs)**\n",
    "\n",
    "- Part 1 of Large Language Models Specialization\n",
    "- Course Length: 9 hours\n",
    "- Last Updated: August 2023\n",
    "\n",
    "---\n",
    "\n",
    "Developed by Algoritma's Research and Development division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07df12",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Background](#toc1_1_)    \n",
    "- [Python Programming Basics for Large Language Models (LLMs)](#toc2_)    \n",
    "  - [Training Objective](#toc2_1_)    \n",
    "  - [Python Programming Evironment Setup](#toc2_2_)    \n",
    "    - [Introduction to Python programming language](#toc2_2_1_)    \n",
    "    - [Installing Python using Miniconda](#toc2_2_2_)    \n",
    "    - [Setting-up the Virtual Environment](#toc2_2_3_)    \n",
    "  - [Jupyter Notebook](#toc2_3_)    \n",
    "    - [Markdown and Code Cells](#toc2_3_1_)    \n",
    "    - [Command Mode and Edit Mode](#toc2_3_2_)    \n",
    "  - [Introduction to Python for Language Preprocessing](#toc2_4_)    \n",
    "    - [Basic Python Programming](#toc2_4_1_)    \n",
    "      - [Variables and Keywords](#toc2_4_1_1_)    \n",
    "      - [Python Data Types](#toc2_4_1_2_)    \n",
    "      - [Dive Deeper: Python Data Types](#toc2_4_1_3_)    \n",
    "      - [Python Data Structures](#toc2_4_1_4_)    \n",
    "      - [Dive Deeper: dictionaries](#toc2_4_1_5_)    \n",
    "      - [Python Functions](#toc2_4_1_6_)    \n",
    "  - [Introduction to Libraries](#toc2_5_)    \n",
    "    - [Implementation of Importing Classes and Functions Using Transformers](#toc2_5_1_)    \n",
    "    - [Dive Deeper: Using Hugging Face Transformers for Text Generation](#toc2_5_2_)    \n",
    "  - [Basics of Language Processing](#toc2_6_)    \n",
    "    - [Using `NLTK` dan `spaCy` for simple text processing](#toc2_6_1_)    \n",
    "      - [Importing the Required Libraries](#toc2_6_1_1_)    \n",
    "      - [Preprocessing the Text](#toc2_6_1_2_)    \n",
    "      - [Lemmatization or Stemming (Optional)](#toc2_6_1_3_)    \n",
    "      - [Named Entity Recognition (NER) using spaCy (Optional)](#toc2_6_1_4_)    \n",
    "  - [Reading External Data using Pandas](#toc2_7_)    \n",
    "    - [Reading `*.csv` Files](#toc2_7_1_)    \n",
    "    - [Reading SQLite Databases](#toc2_7_2_)    \n",
    "  - [Database Connection](#toc2_8_)    \n",
    "- [Summary](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c53aa",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Background](#toc0_)\n",
    "\n",
    "The coursebook is part of the **Large Language Models Specialization** developed by [Algoritma](https://algorit.ma/). The coursebook is intended for a restricted audience only, i.e. the individuals and organizations having received this coursebook directly from the training organization. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b7d63",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Python Programming Basics for Large Language Models (LLMs)](#toc0_)\n",
    "\n",
    "## <a id='toc2_1_'></a>[Training Objective](#toc0_)\n",
    "\n",
    "Generative AI has revolutionized various industries, offering innovative solutions and driving advancements in natural language understanding. Throughout this module, we will delve into the concept of LLM, its applications in diverse business industries, and the ethical considerations associated with its use. We will witness the real-world impact of LLM through engaging demonstrations in different business contexts. Additionally, we will set up the development environment, with Python as the primary programming language, to equip you with the necessary skills for this training. Before diving into the core discussions, we will lay the groundwork by covering Python basics for language preprocessing, introducing the fundamentals of natural language processing, and exploring essential text libraries.\n",
    "\n",
    "\n",
    "- **Python Programming Environment Setup**\n",
    "   - Introduction to Python programming language\n",
    "   - Installing Python using Miniconda\n",
    "   - Setting up a virtual environment\n",
    "   - Using package managers like pip to install libraries\n",
    "\n",
    "\n",
    "- **Jupyter Notebooks: Your Coding Playground**\n",
    "   - Introduction to Jupyter Notebooks for interactive coding and documentation\n",
    "   - Creating and navigating Jupyter Notebook files\n",
    "   - Writing and executing Python code cells\n",
    "   - Markdown cells for adding explanations and text\n",
    "\n",
    "\n",
    "- **Introduction to Python for Language Preprocessing**\n",
    "   - Python basics for beginners\n",
    "   - Variables, data types, basic operations, and functions in Python\n",
    "   - Control structures: loops\n",
    "\n",
    "\n",
    "- **Basics of Language Processing**\n",
    "   - Introduction to Natural Language Processing (NLP)\n",
    "   - Exploring word embeddings and their role in language models\n",
    "   - Introduction to major text libraries in Python (e.g., NLTK, spaCy)\n",
    "   - Understanding text preprocessing and tokenization\n",
    "   - Demonstration of library usage for simple text tasks\n",
    "\n",
    "- **Reading External Data using Pandas**\n",
    "   - Read Data from CSV Files\n",
    "   - Connect to SQL Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f83e5f",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Python Programming Evironment Setup](#toc0_)\n",
    "\n",
    "### <a id='toc2_2_1_'></a>[Introduction to Python programming language](#toc0_)\n",
    "\n",
    "In this lesson, we'll introduce you to the Python programming language, which is a crucial tool for working with Large Language Models (LLMs). Python's simplicity, readability, and extensive libraries make it an ideal choice for both beginners and experienced programmers alike.\n",
    "\n",
    "- **What is Python?**\n",
    "Python is a high-level, interpreted programming language known for its clear and concise syntax. It emphasizes code readability and reduces the cost of program maintenance. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\n",
    "\n",
    "- **Why Python for LLMs?**\n",
    "\n",
    "    + Python's extensive libraries provide robust tools for data manipulation, analysis, and machine learning, making it well-suited for working with text data, a fundamental component of LLMs.\n",
    "    + The community-driven nature of Python development means that there's a rich ecosystem of libraries, frameworks, and resources available for NLP and AI tasks.\n",
    "    + Python's easy-to-learn syntax makes it accessible for individuals with varying levels of programming experience, making it an ideal choice for learners new to programming and LLMs.\n",
    "\n",
    "\n",
    "### <a id='toc2_2_2_'></a>[Installing Python using Miniconda](#toc0_)\n",
    "\n",
    "**1. Installing Python**\n",
    "\n",
    "To use Python, we first need to install it on our system. One convenient way to do this is by using Miniconda, a minimal installer for the Conda package manager. Conda simplifies package management and environment creation, which is crucial for maintaining consistent development environments.\n",
    "\n",
    "**2. Installing Miniconda**\n",
    "\n",
    "Miniconda is a lightweight installer that includes only Conda and its dependencies, making it easier to manage Python packages and environments.\n",
    "Visit the Miniconda website (https://docs.conda.io/en/latest/miniconda.html) and download the installer appropriate for your operating system (Windows, macOS, Linux).\n",
    "\n",
    "\n",
    "**3. Installing Python using Miniconda**\n",
    "\n",
    "Run the downloaded installer and follow the installation instructions. This will set up the Conda package manager along with the Python interpreter.\n",
    "After installation, open a new terminal or command prompt to verify that Conda is installed. Type conda --version to check the Conda version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a932b4",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[Setting-up the Virtual Environment](#toc0_)\n",
    "\n",
    "Conda allows us to create isolated environments for our projects, each with its own dependencies. This is essential for managing packages and avoiding conflicts. A Conda virtual environment is a self-contained directory that holds a **specific collection** of packages and their dependencies. It allows us to create isolated environments for different projects, each with its own set of libraries, versions, and dependencies. This is particularly useful for managing the complexity of software development, ensuring that our projects don't interfere with each other and avoiding conflicts between package versions.\n",
    "\n",
    "**Why Use Conda Virtual Environments?**\n",
    "\n",
    "<img title=\"llm problem\" src=\"assets/environment.png\" width=\"70%\">\n",
    "\n",
    "- **Isolation:** Each virtual environment is isolated from your system's global environment and other virtual environments. This means you can have different versions of packages installed for different projects without conflicts.\n",
    "- **Dependency Management:** Conda handles package dependencies automatically. When you install a package in a virtual environment, Conda ensures that all necessary dependencies are also installed.\n",
    "- **Portability:** Virtual environments can be easily shared with others. By sharing the environment configuration (in a `requirements.txt` file, for instance), collaborators can replicate the same environment on their systems.\n",
    "\n",
    "\n",
    "To setting-up the virtual environment you can follow these steps:\n",
    "\n",
    "> **1. Open Anaconda Prompt** (Windows) or Terminal (macOS/Linux): Launch the Anaconda Prompt or Terminal, which provides a command-line interface for executing Conda-related commands.\n",
    "> \n",
    "> **2. Create a Virtual Environment**: In the Anaconda Prompt or Terminal, use the following command to create a new virtual environment named `llm_env` (you can replace `llm_env` with your desired environment name):\n",
    "> \n",
    "> <div class=\"alert alert-success\">\n",
    ">   <code>conda create --name llm_env python=3.10</code>\n",
    "> </div> \n",
    "> \n",
    "> **3. Activate the Environment:** Activate the Virtual Environment: Once the virtual environment is created, activate it using the following command:\n",
    "> \n",
    "> <div class=\"alert alert-success\">\n",
    ">   <code>conda activate llm_env</code>\n",
    "> </div> \n",
    "> \n",
    "> **4. Install Dependencies** from `requirements.txt`: If you have a `requirements.txt` file that contains a list of dependencies, you can install them into your virtual environment using the following command:\n",
    "> \n",
    "> <div class=\"alert alert-success\">\n",
    ">   <code>pip install -r requirements.txt\n",
    "> </code>\n",
    "> </div> \n",
    "> \n",
    "> Make sure the `requirements.txt` file is present in the directory where you are executing the command. This command will install all the dependencies specified in the file.\n",
    "\n",
    "> **5. Launch Visual Studio Code**: After installing the dependencies, you can launch Jupyter Notebook by executing the following command in the Anaconda Prompt or Terminal: \n",
    "> \n",
    "> <div class=\"alert alert-success\">\n",
    ">   <code>jupyter notebook\n",
    "> </code>\n",
    "> </div> \n",
    "> \n",
    ">> or you can use [Visual Studio Code](https://code.visualstudio.com/) by open the folder directory.\n",
    "> \n",
    "> **6. Open a Jupyter Notebook File**\n",
    "> \n",
    "> Open the Jupyter Notebook file (`*.ipynb`) you want to work on.\n",
    "> \n",
    "> **7. Select the Kernel**\n",
    "> \n",
    "> At the top-right corner of the notebook editor, you'll see a dropdown menu labeled **\"Kernel\"**. Click on it to select a kernel.\n",
    "> \n",
    "> **8. Choose the Virtual Environment**\n",
    "> \n",
    "> From the dropdown list, you should see a list of available kernels, including those from your virtual environments. Look for \"`llm_dev`\" in the list. If you don't see it, make sure you've installed the required packages and set up the virtual environment correctly.\n",
    "\n",
    "\n",
    "<img title=\"llm problem\" src=\"assets/conda_env.png\" width=\"70%\">\n",
    "\n",
    "By following these steps, you'll be able to connect to the kernel of the \"`llm_dev`\" virtual environment in Visual Studio Code's Jupyter Notebook. This ensures that the notebook runs within the isolated environment, allowing you to utilize the packages and configurations specific to that environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Jupyter Notebook](#toc0_)\n",
    "\n",
    "Jupyter Notebooks are powerful and interactive coding environments that allow you to combine code, text explanations, visualizations, and more within a single document. They're widely used in data science, machine learning, and various other fields where code and explanations go hand in hand.\n",
    "\n",
    "Let's delve into Markdown and Code cells in Jupyter Notebooks, as well as the concept of Command Mode and Edit Mode for interacting with cells.\n",
    "\n",
    "### <a id='toc2_3_1_'></a>[Markdown and Code Cells](#toc0_)\n",
    "\n",
    "Jupyter Notebooks allow you to work with two primary types of cells: Markdown cells and Code cells.\n",
    "\n",
    "1. **Markdown Cells:**\n",
    "   - Markdown cells are used for adding text explanations, headings, lists, images, links, and more using Markdown syntax.\n",
    "   - To create a Markdown cell:\n",
    "     - In Command Mode (press `Esc` if you're in Edit Mode), press `B` to insert a new cell below the current cell.\n",
    "     - Then, change the cell type to \"Markdown\" using the cell type dropdown in the toolbar or press `M` in Command Mode.\n",
    "   - You can start typing Markdown-formatted text in the cell.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa6474",
   "metadata": {},
   "source": [
    "This is markdown cell. You can write a formatted text such as **bold** or *italic*. You can even write mathematical formula such"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f75845",
   "metadata": {},
   "source": [
    "- [Markdown Cheatsheet](https://www.markdownguide.org/cheat-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc134a05",
   "metadata": {},
   "source": [
    "2. **Code Cells:**\n",
    "   - Code cells are where you write and execute Python code.\n",
    "   - To create a Code cell:\n",
    "     - In Command Mode, press `B` to insert a new cell below the current cell.\n",
    "     - The cell type is usually \"Code\" by default, but you can also change it using the cell type dropdown in the toolbar or press `Y` in Command Mode.\n",
    "   - Write your Python code within the cell.\n",
    "   - To execute a Code cell, press `Shift + Enter` or click the \"Run\" button in the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2bcc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and this is code cell where you put your python codes down\n",
      "cheers!\n"
     ]
    }
   ],
   "source": [
    "print('and this is code cell where you put your python codes down')\n",
    "print('cheers!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4e811",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[Command Mode and Edit Mode](#toc0_)\n",
    "Jupyter Notebooks operate in two main modes: Command Mode and Edit Mode. These modes allow you to perform different actions on cells and within the notebook interface.\n",
    "\n",
    "1. **Command Mode:**\n",
    "   - In Command Mode, you can manipulate cells at a higher level. You're not editing the content within cells; instead, you're interacting with the cells themselves.\n",
    "   - Pressing `Esc` or clicking outside a cell activates Command Mode.\n",
    "   - Common Command Mode actions:\n",
    "     - Creating new cells (`A` for above, `B` for below)\n",
    "     - Changing cell types (`Y` for Code, `M` for Markdown)\n",
    "     - Deleting cells (`D, D`)\n",
    "     - Moving cells (`Shift + M` to merge, `M` to split)\n",
    "     - Saving the notebook (`S`)\n",
    "     - Cut selected cell (`x`)\n",
    "     - Copy selected cell (`c`)\n",
    "     - Paste selected cell (`v`)\n",
    "     - Undo command mode (`z`)\n",
    "\n",
    "2. **Edit Mode:**\n",
    "   - In Edit Mode, you're actively editing the content within a cell (either Markdown or Code).\n",
    "   - Clicking inside a cell activates Edit Mode.\n",
    "   - Common Edit Mode actions:\n",
    "     - Writing and editing code or Markdown text\n",
    "     - Using keyboard shortcuts for faster typing and navigation within the cell\n",
    "\n",
    "3. **Works in both Command or Edit mode**\n",
    "  - `shift + enter` : run selected cell and move to the next cell below (add new if doesn't exist)\n",
    "\n",
    "\n",
    "Remember, Jupyter Notebooks are designed to provide a seamless combination of code and explanations. You can switch between Command Mode and Edit Mode to perform different tasks efficiently. Whether you're writing code, adding explanations, or visualizing results, Jupyter Notebooks offer a versatile and interactive environment for your data analysis and coding needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090ed9c",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Introduction to Python for Language Preprocessing](#toc0_)\n",
    "\n",
    "Python is a powerful programming language that offers a wide range of tools and libraries for language preprocessing tasks in the field of natural language processing (NLP). This section provides an overview of Python's essential concepts and features relevant to language preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84547ae",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_1_'></a>[Basic Python Programming](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_1_'></a>[Variables and Keywords](#toc0_)\n",
    "\n",
    "In Python, variables are used to store data values. They serve as containers for holding values that can be referenced and manipulated throughout the program.\n",
    "\n",
    "- **Variable Declaration**: To declare a variable in Python, you simply assign a value to it using the assignment operator `(=)`. The variable name should be meaningful and follow certain naming conventions (e.g., start with a letter or underscore, avoid using reserved keywords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b813cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programming\n"
     ]
    }
   ],
   "source": [
    "activity = 'programming'\n",
    "\n",
    "print(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5338f",
   "metadata": {},
   "source": [
    " Thing to note here, like other programming languages, Python is **case-sensitive**, so `activity` and `Activity` are  different symbols and will point to different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37083638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.665404Z",
     "start_time": "2023-07-07T10:28:25.660889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'activity' == 'Activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321cfda3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.672421Z",
     "start_time": "2023-07-07T10:28:25.668143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " activity == activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c1f63",
   "metadata": {},
   "source": [
    "Our previous code returned `True` as the output. Try to create a new variable and use `True` as the variable name, then see what happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c25c2",
   "metadata": {},
   "source": [
    "**Keywords** in Python are reserved words that have specific meanings and purposes within the language. These keywords **cannot be used** as variable names because they are already used by Python to perform specific tasks or operations.\n",
    "Examples of keywords: `if`, `else`, `for`, `while`, `def`, `import`, `return`, `class`, `True`, `False`, `None`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_2_'></a>[Python Data Types](#toc0_)\n",
    "\n",
    "In Python, data types represent the kind of values that variables can hold. Each data type has its own characteristics and behavior. Here's an explanation of some commonly used data types in Python along with examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bb6bb",
   "metadata": {},
   "source": [
    "**1. Numeric Types:**\n",
    "\n",
    "To store numbers, python has two native data types called `int` and `float`.\n",
    "\n",
    "- `int` is used to store integers (ie: 1,2,-3)\n",
    "- `float` is used to store real numbers (ie: 0.7, -1.8, -1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f7b64d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.678508Z",
     "start_time": "2023-07-07T10:28:25.674391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int\n",
    "age = 25\n",
    "type(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40d596e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.686226Z",
     "start_time": "2023-07-07T10:28:25.681774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float\n",
    "weight = 68.5\n",
    "type(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f845cc5",
   "metadata": {},
   "source": [
    "**Numeric Operations** \n",
    "\n",
    "Arithmetic Operators:\n",
    "\n",
    "- `+` - Addition\n",
    "- `-` - Subtraction\n",
    "- `*` - Multiplication\n",
    "- `/` - Division\n",
    "- `//` - Round division\n",
    "- `%` - Module\n",
    "- `**` - Exponent\n",
    "\n",
    "Comparison Operators:\n",
    "\n",
    "- `<` - Less than (ie : a < b)\n",
    "- `<=` - Less than or equal to (ie : a <= b)\n",
    "- `>` - Greater than (ie: a > b)\n",
    "- `>=` - Greater than or equal to (ie: a >= b)\n",
    "- `==` - Equals (ie: a == b)\n",
    "- `!=` - Not Equal (ie: a != b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4025181",
   "metadata": {},
   "source": [
    "**2. Strings**\n",
    "\n",
    "Strings are used in Python to record text information, such as names. Strings in Python are actually a sequence, which basically means Python keeps track of every element in the string as a sequence. For example, Python understands the string \"hello' to be a sequence of letters in a specific order. This means we will be able to use indexing to grab particular letters (like the first letter, or the last letter).\n",
    "\n",
    "Python represents any string as a `str` object. There are several ways to create a string value:\n",
    "\n",
    "- using `''` (ie: `'cyber punk 2077'``)\n",
    "- using `\"\"` (ie : `\"Hari Jum'at\"`)\n",
    "- using `'''` or `\"\"\"` (ie: `'''Andi berkata \"Jum'at Bersih\"'''`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a22c07b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.693000Z",
     "start_time": "2023-07-07T10:28:25.688277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str\n",
    "school = \"Algoritma\"\n",
    "type(school)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a07ba",
   "metadata": {},
   "source": [
    "**3. Boolean**\n",
    "\n",
    "Boolean stores a very simple value in computers and programming, `True` or `False`.\n",
    "\n",
    "**Boolean operations**\n",
    "\n",
    "Python provides logical operators such as:\n",
    "\n",
    "- and (ie: a and b)\n",
    "- or (ie: a or c)\n",
    "- not (ie: not a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91f3b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boolean\n",
    "is_student = True\n",
    "type(is_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b5fee",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_3_'></a>[Dive Deeper: Python Data Types](#toc0_)\n",
    "\n",
    "1. Calculate the result of the following expression and assign it to the variable 'result'. Then, print the data type of 'result'.\n",
    "\n",
    "    ```\n",
    "    (5 * 3) + (10 / 2)\n",
    "    ```\n",
    "\n",
    "2. Evaluate the expression (10 > 5) and assign the result to the variable 'is_greater'. Print whether 'is_greater' is True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_4_'></a>[Python Data Structures](#toc0_)\n",
    "\n",
    "Python provides several built-in data structures that allow you to organize and store collections of data. These data structures are essential for efficient data manipulation and are widely used in Python programming. Here's an explanation of some commonly used data structures along with examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331701f3",
   "metadata": {},
   "source": [
    "**1. List**\n",
    "\n",
    "Lists are ordered collections of items enclosed in square brackets (`[]`). They can store elements of different data types and allow duplicate values. Lists support indexing and slicing, which enable you to access and manipulate specific elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408d766f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.705389Z",
     "start_time": "2023-07-07T10:28:25.701893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n"
     ]
    }
   ],
   "source": [
    "fruits = [\"apple\", \"banana\", \"orange\"]\n",
    "print(fruits[0])  # Output: \"apple\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d70298",
   "metadata": {},
   "source": [
    "**Operation List**\n",
    "\n",
    "- `x.append(a)` : add a to x\n",
    "- `x.remove(a)` : remove a from x\n",
    "\n",
    "In addition to the previously known operators, one of the most useful lists is to implement an aggregation function such as:\n",
    "\n",
    "- `len(x)` : extract the length of the list\n",
    "- `a in b` : checks if the value `a` exists in the list object `b`\n",
    "- `max(x)` : get the highest value in x\n",
    "- `sum(x)` : get the number of values in x\n",
    "\n",
    "Another operation to be aware of in lists is indexing:\n",
    "\n",
    "- `x[i]` : access the i-th element of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ea2e4",
   "metadata": {},
   "source": [
    "**2. Dictionaries**\n",
    "\n",
    "Dictionaries store data in key-value pairs enclosed in curly braces (`{}`). Each element in a dictionary consists of a unique key and its corresponding value. Dictionaries provide fast lookup operations based on keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a3bd1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.725001Z",
     "start_time": "2023-07-07T10:28:25.722042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a dictionary with {} and : to signify a key and a value\n",
    "my_dict = {'key1':'value1',\n",
    "           'key2':'value2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96bc8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.731131Z",
     "start_time": "2023-07-07T10:28:25.727049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'value2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call values by their key\n",
    "my_dict['key2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66a1ca",
   "metadata": {},
   "source": [
    "Some common operations and methods for dictionaries in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b981309",
   "metadata": {},
   "source": [
    "- Accessing Values: Dictionaries use keys to access corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f4d3bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.736690Z",
     "start_time": "2023-07-07T10:28:25.733357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n"
     ]
    }
   ],
   "source": [
    "student = {\"name\": \"John\", \"age\": 20}\n",
    "print(student[\"name\"])     # Output: \"John\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ee3dc",
   "metadata": {},
   "source": [
    "- Modifying Values: You can modify the values of a dictionary by assigning a new value to a specific key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64956b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.742365Z",
     "start_time": "2023-07-07T10:28:25.739186Z"
    }
   },
   "outputs": [],
   "source": [
    "student = {\"name\": \"John\", \"age\": 20}\n",
    "student[\"age\"] = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b735f",
   "metadata": {},
   "source": [
    "- Adding and Removing Key-Value Pairs: You can add new key-value pairs to a dictionary using the assignment operator, and remove key-value pairs using the `del` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14468cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.747564Z",
     "start_time": "2023-07-07T10:28:25.744592Z"
    }
   },
   "outputs": [],
   "source": [
    "student = {\"name\": \"John\", \"age\": 20}\n",
    "student[\"grade\"] = \"A\"     # Adding a new key-value pair\n",
    "del student[\"age\"]         # Removing a key-value pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1c061",
   "metadata": {},
   "source": [
    "Dictionary Methods: Dictionaries have several useful methods, such as `keys()`, `values()`, and `items()`, which return the keys, values, and key-value pairs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe67e240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.752800Z",
     "start_time": "2023-07-07T10:28:25.749793Z"
    }
   },
   "outputs": [],
   "source": [
    "student = {\"name\": \"John\", \"age\": 20}\n",
    "keys = student.keys()\n",
    "values = student.values()\n",
    "items = student.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d362329",
   "metadata": {},
   "source": [
    "- Checking Key Existence: You can use the `in` keyword to check if a key exists in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c272d306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age is present in the dictionary\n"
     ]
    }
   ],
   "source": [
    "student = {\"name\": \"John\", \"age\": 20}\n",
    "if \"age\" in student:\n",
    "    print(\"Age is present in the dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_5_'></a>[Dive Deeper: dictionaries](#toc0_)\n",
    "\n",
    "Create a dictionary 'person' with the following key-value pairs:\n",
    "\n",
    "    ```\n",
    "    'name': 'Alice', 'age': 30, 'city': 'New York'\n",
    "    ```\n",
    "Update the 'age' to 31 and print the updated dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c873621",
   "metadata": {},
   "source": [
    "**for Loops**\n",
    "\n",
    "A `for` loop acts as an *iterator* in Python; it goes through items that are in a sequence or any other iterable item. Objects that we've learned about that we can iterate over include strings, lists, tuples, and even built-in iterables for dictionaries, such as keys or values.\n",
    "\n",
    "We've already seen the `for` statement a little bit in past lectures but now let's formalize our understanding.\n",
    "\n",
    "Here's the general format for a `for` loop in Python:\n",
    "```\n",
    "for item in object:\n",
    "    statements to do stuff\n",
    "```\n",
    "\n",
    "The variable name used for the item is completely up to the coder, so use your best judgment for choosing a name that makes sense and you will be able to understand when revisiting your code. This item name can then be referenced inside your loop, for example if you wanted to use `if` statements to perform checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac3a47b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.774575Z",
     "start_time": "2023-07-07T10:28:25.771030Z"
    }
   },
   "outputs": [],
   "source": [
    "my_list1 = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6d9236c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.779677Z",
     "start_time": "2023-07-07T10:28:25.776714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for num in my_list1:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f386f78",
   "metadata": {},
   "source": [
    "We could have also put an `if` `else` statement in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f6ce4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd number\n",
      "2\n",
      "Odd number\n",
      "4\n",
      "Odd number\n",
      "6\n",
      "Odd number\n",
      "8\n",
      "Odd number\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for num in my_list1:\n",
    "    if num % 2 == 0:\n",
    "        print(num)\n",
    "    else:\n",
    "        print('Odd number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_6_'></a>[Python Functions](#toc0_)\n",
    "\n",
    "A function is a useful device that groups together a set of statements so they can be run more than once. They can also let us specify parameters that can serve as inputs to the functions.\n",
    "\n",
    "On a more fundamental level, functions allow us to not have to repeatedly write the same code again and again. If you remember back to the lessons on strings and lists, remember that we used a function len() to get the length of a string. Since checking the length of a sequence is a common task you would want to write a function that can do this repeatedly at command.\n",
    "\n",
    "**Why even use functions?**\n",
    "\n",
    "Put simply, you should use functions when you plan on using a block of code multiple times. The function will allow you to call the same block of code without having to write it multiple times. This in turn will allow you to create more complex Python scripts. To really understand this though, we should actually write our own functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df8b27",
   "metadata": {},
   "source": [
    " **Creating a function**\n",
    "\n",
    "\n",
    "In Python a function is defined using the `def` keyword, and follow by function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4b63f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.792591Z",
     "start_time": "2023-07-07T10:28:25.789047Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_function():\n",
    "  print(\"Hello from a function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580ff46",
   "metadata": {},
   "source": [
    "**Calling a function**\n",
    "\n",
    "To call a function, use the function name followed by parenthesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fe3d3d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.807030Z",
     "start_time": "2023-07-07T10:28:25.802831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from a function\n"
     ]
    }
   ],
   "source": [
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1416bf0",
   "metadata": {},
   "source": [
    "**Arguments**\n",
    "\n",
    "\n",
    "Information can be passed into functions as arguments.\n",
    "\n",
    "Arguments are specified after the function name, inside the parentheses. You can add as many arguments as you want, just separate them with a comma.\n",
    "\n",
    "The following example has a function with one argument (`name`). When the function is called, we pass along a first name, which is used inside the function to print the full name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34af0ddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.813420Z",
     "start_time": "2023-07-07T10:28:25.809338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwi from Algoritma\n",
      "Irfan from Algoritma\n",
      "Lita from Algoritma\n"
     ]
    }
   ],
   "source": [
    "def my_function(name):\n",
    "  print(name + \" from Algoritma\")\n",
    "\n",
    "my_function('Dwi')\n",
    "my_function('Irfan')\n",
    "my_function('Lita')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b61d2",
   "metadata": {},
   "source": [
    "**Using return**\n",
    "\n",
    "So far we've only seen `print()` used, but if we actually want to save the resulting variable we need to use the **return** keyword.\n",
    "\n",
    "Let's see some example that use a `return` statement. `return` allows a function to *return* a result that can then be stored as a variable, or used in whatever manner a user wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a10dc376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.818516Z",
     "start_time": "2023-07-07T10:28:25.815666Z"
    }
   },
   "outputs": [],
   "source": [
    "def area(width,length):\n",
    "    return width*length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd89be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.825797Z",
     "start_time": "2023-07-07T10:28:25.820941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area(4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8cb79",
   "metadata": {},
   "source": [
    "**A Very Common Question: \"What is the difference between `return` and `print`?\"**\n",
    "\n",
    "> The `return` keyword allows you to actually save the result of the output of a function as a variable. The `print()` function simply displays the output to you, but doesn't save it for future use. Let's explore this in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87bd4692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.830626Z",
     "start_time": "2023-07-07T10:28:25.828031Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_result(a,b):\n",
    "    print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5621df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.836266Z",
     "start_time": "2023-07-07T10:28:25.833152Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_result(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7802d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.842169Z",
     "start_time": "2023-07-07T10:28:25.838456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print_result(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfef6cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.848643Z",
     "start_time": "2023-07-07T10:28:25.844412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You won't see any output if you run this in a .py script\n",
    "return_result(10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76bfe",
   "metadata": {},
   "source": [
    "But what happens if we actually want to save this result for later use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340940c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.853659Z",
     "start_time": "2023-07-07T10:28:25.850736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "my_result = print_result(20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b54cc40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.858444Z",
     "start_time": "2023-07-07T10:28:25.855800Z"
    }
   },
   "outputs": [],
   "source": [
    "my_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b27e8413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.864536Z",
     "start_time": "2023-07-07T10:28:25.860523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087be4b",
   "metadata": {},
   "source": [
    "> Be careful! Notice how `print_result()` doesn't let you actually save the result to a variable! It only prints it out, with `print()` returning `None` for the assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[Introduction to Libraries](#toc0_)\n",
    "\n",
    "**Libraries**\n",
    "\n",
    "A library in programming is a collection of pre-written code, functions, classes, and tools that you can use to perform various tasks without having to write everything from scratch. Libraries provide solutions to common problems and allow you to leverage the expertise of other developers. They can save you time and effort, increase the reliability of your code, and enable you to focus on the unique aspects of your project.\n",
    "\n",
    "Libraries come in various forms, from general-purpose libraries that provide fundamental programming capabilities to domain-specific libraries tailored for specific tasks, such as data analysis, web development, machine learning, and more.\n",
    "\n",
    "**Modules**\n",
    "\n",
    "A module is a single file containing Python code that groups related functions, classes, and variables together. Modules provide a way to organize code, making it more modular, reusable, and maintainable. By breaking your code into modules, you can focus on specific functionalities and keep your codebase organized.\n",
    "\n",
    "Python's modular architecture allows you to create your own modules and use them as building blocks in larger projects. Additionally, Python comes with a rich set of built-in modules (part of the Python Standard Library) that cover a wide range of tasks, from file I/O to math operations.\n",
    "\n",
    "### <a id='toc2_5_1_'></a>[Implementation of Importing Classes and Functions Using Transformers](#toc0_)\n",
    "\n",
    "When we're working with external libraries, like the Hugging Face Transformers library, we need to import the classes, functions, and resources we want to use into our code. This process involves:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78bad8",
   "metadata": {},
   "source": [
    "**1. Importing Class**\n",
    "\n",
    "Importing classes and functions from external modules or libraries is a fundamental concept in programming. Let's break down the implementation of importing classes:\n",
    "\n",
    "To import a class from a module, you use the `from` keyword followed by the module name, `import` keyword, and then the class name. For example:\n",
    "\n",
    "   >```python\n",
    "   > from module_name import ClassName\n",
    "   > ```\n",
    "\n",
    "**2. Importing a Function:**\n",
    "   Similar to importing a class, you can import a function from a module using the `from` keyword, `import` keyword, and the function name. For example:\n",
    "\n",
    "   >```python\n",
    "   >from module_name import function_name\n",
    "   > ```\n",
    "\n",
    "\n",
    "**Example Using Hugging Face Transformers:**\n",
    "\n",
    "Let's say you want to use the `GPT2LMHeadModel` class and the `GPT2Tokenizer` class from the Hugging Face Transformers library to generate text. Here's an example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcd45b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary classes from the Hugging Face Transformers library\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bafcec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'  # You can choose other variants like 'gpt2-medium', 'gpt2-large', etc.\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff484ebd",
   "metadata": {},
   "source": [
    "1. **Model Name:**\n",
    "   In this line, a variable named `model_name` is assigned the string `'gpt2'`. This variable specifies the name of the pre-trained GPT-2 model to be loaded. You can choose different variants of the GPT-2 model based on their size and capabilities. Variants include `'gpt2-small'`, `'gpt2-medium'`, `'gpt2-large'`, and more.\n",
    "\n",
    "2. **Loading the Model:**\n",
    "   The line `model = GPT2LMHeadModel.from_pretrained(model_name)` loads a pre-trained GPT-2 model using the `from_pretrained` method provided by the `GPT2LMHeadModel` class from the Transformers library. This method fetches the pre-trained weights and configuration of the specified model variant (`model_name`) from the Hugging Face model repository and initializes a new instance of the `GPT2LMHeadModel` class with those weights and configuration.\n",
    "\n",
    "3. **Loading the Tokenizer:**\n",
    "   The line `tokenizer = GPT2Tokenizer.from_pretrained(model_name)` loads a pre-trained tokenizer using the `from_pretrained` method provided by the `GPT2Tokenizer` class from the Transformers library. Similar to loading the model, this method fetches the tokenizer configuration and assets for the specified model variant (`model_name`) and initializes a new instance of the `GPT2Tokenizer` class.\n",
    "\n",
    "At the end of this code snippet, you have two variables, `model` and `tokenizer`, that are ready to be used for text generation or any other task the GPT-2 model and tokenizer are designed for. These variables encapsulate the loaded model and tokenizer instances, allowing you to interact with the pre-trained GPT-2 model and process text inputs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5131a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the model and tokenizer\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0e54d",
   "metadata": {},
   "source": [
    "1. **Prompt:**\n",
    "   The variable `prompt` contains the starting text that you want the model to continue generating from. In this case, the prompt is `\"Once upon a time\"`.\n",
    "\n",
    "2. **Encoding Prompt with Tokenizer:**\n",
    "   The line `input_ids = tokenizer.encode(prompt, return_tensors='pt')` encodes the `prompt` text using the tokenizer. The tokenizer converts the input text into numerical IDs (tokens) that the model understands. The `return_tensors='pt'` argument ensures that the encoded input is returned as PyTorch tensors.\n",
    "\n",
    "3. **Generating Output:**\n",
    "   The line `output = model.generate(input_ids, max_length=50, num_return_sequences=1)` uses the loaded `model` to generate text based on the provided `input_ids`. It generates a sequence of tokens that continues from the provided prompt. The `max_length` argument specifies the maximum length of the generated sequence, and `num_return_sequences` specifies the number of different sequences to generate. In this case, we are generating a single sequence.\n",
    "\n",
    "4. **Decoding Output with Tokenizer:**\n",
    "   The line `generated_text = tokenizer.decode(output[0], skip_special_tokens=True)` decodes the generated sequence of tokens back into human-readable text using the `tokenizer`. The `output[0]` contains the generated sequence of token IDs, and `skip_special_tokens=True` ensures that any special tokens (like padding or end-of-sentence tokens) are omitted from the decoded text.\n",
    "\n",
    "At the end of this code snippet, you have the `generated_text` variable that contains the text generated by the pre-trained model. This text continues from the provided prompt and offers a result that the model predicts based on its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f544888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "# Print the generated text\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a795f6a",
   "metadata": {},
   "source": [
    "1. **`print()` Function:**\n",
    "   The `print()` function is a built-in Python function used to display text or other values in the console. It takes one or more arguments and prints them, separated by spaces, to the console output.\n",
    "\n",
    "2. **Arguments to `print()`:**\n",
    "   - `\"Generated Text:\"` is a string that is printed as-is. It acts as a label to indicate what the following text represents.\n",
    "   - `generated_text` is the variable containing the generated text that you want to display. It contains the continuation of text that the model generated based on the provided prompt.\n",
    "\n",
    "3. **Output:**\n",
    "   When this line of code is executed, it will print a message to the console that looks like this (assuming `generated_text` contains the generated text):\n",
    "   ```\n",
    "   Generated Text: Once upon a time, in a land far, far away...\n",
    "   ```\n",
    "\n",
    "By using the `print()` function, you can display the results of your code, such as the generated text in this case, so that you can view and analyze the output. This is a common practice for verifying the correctness of your program's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_2_'></a>[Dive Deeper: Using Hugging Face Transformers for Text Generation](#toc0_)\n",
    "\n",
    "1. Import the required classes and initialize a GPT-2 model and tokenizer. Use the 'gpt2' model variant and store the tokenizer in the variable 'tokenizer' and the model in the variable 'model_new'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3b12512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8ebe9",
   "metadata": {},
   "source": [
    "2. Generate text using the initialized model and tokenizer. Use the prompt \"In a galaxy\" and set the maximum length to 100. Store the generated text in the variable 'generated_text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "012317d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46092906",
   "metadata": {},
   "source": [
    "3. Decode and print the generated text without special tokens. Use the 'generated_text' variable from the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b7c5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198e40f",
   "metadata": {},
   "source": [
    "4. Generate text with a custom prompt and length. Use the prompt \"Once upon a time, there was\" and set the maximum length to 150. Store the generated text in the variable 'custom_generated_text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce147f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e259ff",
   "metadata": {},
   "source": [
    "5. Count the number of tokens in the 'custom_generated_text' using the tokenizer. Store the count in the variable 'token_count'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4b7e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ed4b8",
   "metadata": {},
   "source": [
    "This dive deeper helps learners practice importing classes, initializing models and tokenizers, generating text, and performing basic text analysis using the Hugging Face Transformers library. Make sure to provide the quiz questions in a format where learners can fill in the code blanks and see the results immediately to enhance their learning experience.\n",
    "<!--\n",
    "**Question 1:**\n",
    "```python\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "**Question 2:**\n",
    "```python\n",
    "prompt = \"In a galaxy\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "**Question 3:**\n",
    "```python\n",
    "print(\"Generated Text:\", generated_text)\n",
    "```\n",
    "\n",
    "**Question 4:**\n",
    "```python\n",
    "custom_prompt = \"Once upon a time, there was\"\n",
    "custom_input_ids = tokenizer.encode(custom_prompt, return_tensors='pt')\n",
    "custom_output = model.generate(custom_input_ids, max_length=150, num_return_sequences=1)\n",
    "custom_generated_text = tokenizer.decode(custom_output[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "**Question 5:**\n",
    "```python\n",
    "token_count = len(tokenizer.encode(custom_generated_text))\n",
    "```\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a948b606",
   "metadata": {},
   "source": [
    "## <a id='toc2_6_'></a>[Basics of Language Processing](#toc0_)\n",
    "\n",
    "Language processing, also known as **natural language processing (NLP)**, is a field of study that focuses on the interaction between computers and human language. It involves the development of algorithms and techniques to enable computers to understand, interpret, and generate human language in a way that is meaningful and useful.\n",
    "\n",
    "The field of language processing encompasses a wide range of tasks, including but not limited to:\n",
    "\n",
    "1. **Tokenization**: Breaking down a text into smaller units, such as words or sentences, known as tokens.\n",
    "\n",
    "2. **Part-of-Speech (POS) Tagging**: Assigning grammatical tags to words in a sentence, indicating their part of speech (e.g., noun, verb, adjective).\n",
    "\n",
    "3. **Named Entity Recognition (NER)**: Identifying and classifying named entities in text, such as person names, locations, organizations, or dates.\n",
    "\n",
    "4. **Sentiment Analysis**: Determining the sentiment or emotional tone expressed in a piece of text, such as positive, negative, or neutral.\n",
    "\n",
    "5. **Text Classification**: Categorizing text into predefined categories or classes based on its content or topic.\n",
    "\n",
    "6. **Language Generation**: Generating human-like text based on given input or prompts.\n",
    "\n",
    "7. **Machine Translation**: Translating text from one language to another.\n",
    "\n",
    "8. **Information Extraction**: Extracting structured information from unstructured text, such as extracting names, dates, or relations from news articles.\n",
    "\n",
    "These are just a few examples of the tasks involved in language processing. Python provides various libraries and tools, such as NLTK (Natural Language Toolkit), spaCy, and scikit-learn, which offer functionalities and pre-trained models to perform these tasks efficiently.\n",
    "\n",
    "By understanding the basics of language processing, you can lay the foundation for more advanced applications, including large language models (LLM), which utilize complex algorithms and deep learning techniques to process and generate human-like language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3a071",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_1_'></a>[Using `NLTK` dan `spaCy` for simple text processing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_6_1_1_'></a>[Importing the Required Libraries](#toc0_)\n",
    "\n",
    "Begin by importing the necessary libraries for text processing, such as `NLTK` and `spaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a202b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_6_1_2_'></a>[Preprocessing the Text](#toc0_)\n",
    "\n",
    "Perform basic text preprocessing tasks, such as tokenization and removing stop words. Tokenization is a crucial step in natural language processing tasks as it breaks down text into smaller units for further analysis, processing, or modeling. Removing stop words helps eliminate noise and focus on more meaningful words when performing text analysis, classification, or other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc77c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Removing stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad649262",
   "metadata": {},
   "source": [
    "1. Tokenization:\n",
    "   - The variable `text` contains a sample sentence: \"This is a sample sentence.\"\n",
    "   - The `word_tokenize()` function from NLTK is used to tokenize the sentence into individual words.\n",
    "   - The result is stored in the `tokens` variable, which will contain a list of tokens (words) from the sentence.\n",
    "   \n",
    "\n",
    "2. Removing Stop Words:\n",
    "   - Stop words are common words that do not carry significant meaning in a sentence, such as \"is,\" \"a,\" \"the,\" etc.\n",
    "   - NLTK provides a predefined set of stop words for different languages, including English.\n",
    "   - The `stopwords.words(\"english\")` function retrieves the set of English stop words.\n",
    "   - The set of stop words is stored in the `stop_words` variable.\n",
    "   - A list comprehension is used to create a new list called `filtered_tokens`.\n",
    "   - Each token in the `tokens` list is checked against the set of stop words.\n",
    "   - If a token, when converted to lowercase, is not present in the stop words set, it is included in the `filtered_tokens` list.\n",
    "   - The resulting `filtered_tokens` list will contain only the tokens from the original sentence that are not considered stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef170275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.881775Z",
     "start_time": "2023-07-07T10:28:25.878333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a sample sentence.\n",
      "Tokens: ['This', 'is', 'a', 'sample', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\", text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc511ad3",
   "metadata": {},
   "source": [
    "Tokens: The text is tokenized into individual words or punctuation marks. The tokens for the given text are ['This', 'is', 'a', 'sample', 'sentence', '.']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_6_1_3_'></a>[Lemmatization or Stemming (Optional)](#toc0_)\n",
    "\n",
    "Apply lemmatization or stemming to reduce words to their base or root form. Both lemmatization and stemming help in reducing variations of words to their base forms, which can be useful for tasks such as information retrieval, text analysis, or language modeling. Choosing between lemmatization and stemming depends on the specific requirements of your application or task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf64d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0cd87d",
   "metadata": {},
   "source": [
    "1. Lemmatization:\n",
    "   - The line `lemmatizer = nltk.stem.WordNetLemmatizer()` creates an instance of the WordNetLemmatizer class from the NLTK library.\n",
    "   - Lemmatization is the process of reducing words to their base or root form (lemmas) to improve analysis or comparison.\n",
    "   - The list comprehension `[lemmatizer.lemmatize(token) for token in filtered_tokens]` applies lemmatization to each token in the `filtered_tokens` list.\n",
    "   - The lemmatized tokens are stored in the `lemmatized_tokens` list.\n",
    "\n",
    "2. Stemming:\n",
    "   - The line `stemmer = nltk.stem.PorterStemmer()` creates an instance of the PorterStemmer class from the NLTK library.\n",
    "   - Stemming is the process of reducing words to their base or root form by removing suffixes.\n",
    "   - The list comprehension `[stemmer.stem(token) for token in filtered_tokens]` applies stemming to each token in the `filtered_tokens` list.\n",
    "   - The stemmed tokens are stored in the `stemmed_tokens` list.\n",
    "\n",
    "The goal of this code is to showcase two different text normalization techniques: lemmatization and stemming.\n",
    "\n",
    "- Lemmatization aims to obtain the base or root form of words. For example, the lemma of \"running\" is \"run\" and the lemma of \"better\" is \"good\".\n",
    "- Stemming, on the other hand, reduces words to their base form by removing common suffixes. For example, stemming \"running\" would result in \"run\" and stemming \"better\" would become \"bet\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "481cba05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:25.893180Z",
     "start_time": "2023-07-07T10:28:25.889987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['sample', 'sentence', '.']\n",
      "Lemmatized Tokens: ['sample', 'sentence', '.']\n",
      "Stemmed Tokens: ['sampl', 'sentenc', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82d7a8",
   "metadata": {},
   "source": [
    "- Lemmatized Tokens: The filtered tokens are lemmatized, meaning they are reduced to their base or dictionary form. In this case, since the tokens don't have inflectional endings, the lemmatized tokens remain the same as the filtered tokens: ['sample', 'sentence', '.'].\n",
    "\n",
    "- Stemmed Tokens: The filtered tokens are stemmed, meaning they are reduced to their root form by removing suffixes. In this case, the stemmed tokens are ['sampl', 'sentenc', '.']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_6_1_4_'></a>[Named Entity Recognition (NER) using spaCy (Optional)](#toc0_)\n",
    "\n",
    "Perform named entity recognition to extract entities from the text. This information can be useful in various applications, such as information extraction, question-answering systems, or data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa852d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7272547a",
   "metadata": {},
   "source": [
    "1. Loading the Language Model:\n",
    "   - The line `nlp = spacy.load(\"en_core_web_sm\")` loads the English language model from spaCy. This model includes pre-trained word vectors, syntax, entities, and other linguistic annotations.\n",
    "\n",
    "2. Processing the Text:\n",
    "   - The line `doc = nlp(text)` processes the input text using the loaded language model. The `text` variable contains the text you want to analyze.\n",
    "   - The `nlp` object processes the text and creates a `doc` object that contains the analyzed information, such as tokens, part-of-speech tags, syntactic dependencies, and named entities.\n",
    "\n",
    "3. Extracting Named Entities:\n",
    "   - Named entity recognition (NER) is a natural language processing task that aims to locate and classify named entities in text.\n",
    "   - The line `entities = [(entity.text, entity.label_) for entity in doc.ents]` extracts the named entities from the `doc` object.\n",
    "   - The list comprehension retrieves the text and label of each named entity in the `doc.ents` attribute.\n",
    "   - The `entities` variable stores the extracted named entities as tuples, where each tuple contains the entity text and its corresponding label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5fe7406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T10:28:26.550260Z",
     "start_time": "2023-07-07T10:28:26.546792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Named Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdaa0e",
   "metadata": {},
   "source": [
    "Named Entities: No named entities were detected in the original text, so the list of named entities is empty: []."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_7_'></a>[Reading External Data using Pandas](#toc0_)\n",
    "\n",
    "For our upcoming courses and projects, we'll be delving into the practical application of Large Language Models (LLMs) using external data sources. This will include working with diverse types of tabular data, such as **CSV files, text files (TXT), and SQLite databases**. To seamlessly integrate these data sources with our LLM workflows, we'll leverage one of the most widely-used libraries in Python: pandas.\n",
    "\n",
    "Pandas is an essential tool for data manipulation and analysis. With its intuitive data structures like DataFrames and Series, pandas offers a versatile and efficient way to read, clean, transform, and analyze data. This library empowers us to effortlessly load external data, prepare it for LLM tasks, and derive valuable insights.\n",
    "\n",
    "Through `pandas``, we can effortlessly:\n",
    "\n",
    "1. **Read Data from Various Sources:**\n",
    "   Whether it's CSV files containing structured data, raw text files, or SQLite databases housing relational information, pandas provides dedicated functions like `read_csv()`, `read_table()`, and `read_sql_query()` to fetch data into the familiar DataFrame format. This unified approach enables a seamless transition from external data to analysis-ready formats.\n",
    "\n",
    "2. **Clean and Transform Data:**\n",
    "   Data often requires cleaning and preprocessing before feeding it into LLMs. Pandas equips us with tools to handle missing values, filter and sort data, perform arithmetic operations, and reshape datasets, streamlining the data preparation process.\n",
    "\n",
    "3. **Integrate with LLM Tasks:**\n",
    "   After loading data, pandas allows us to apply LLM-specific tasks. We can tokenize text, convert it into numerical format, and structure it into sequences that LLMs can understand. Additionally, we can create custom functions to preprocess data further and handle specific LLM input requirements.\n",
    "\n",
    "4. **Facilitate Exploratory Analysis:**\n",
    "   Exploring data is a crucial step before implementing LLMs. Pandas lets us quickly compute descriptive statistics, generate visualizations, and understand the characteristics of our dataset. This exploration aids in identifying patterns, trends, and potential areas for LLM application.\n",
    "\n",
    "In our journey of merging external data and LLMs, pandas serves as a bridge that connects these two realms seamlessly. Its capabilities empower us to efficiently prepare data for LLM tasks, maximizing the quality and relevance of the insights we can extract. By harnessing the power of pandas alongside Large Language Models, we unlock a wealth of possibilities for understanding, transforming, and generating meaningful content from diverse external datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed168375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_1_'></a>[Reading `*.csv` Files](#toc0_)\n",
    "\n",
    "CSV (Comma-Separated Values) files are a common format for storing structured data. Pandas provides the `read_csv()` function to read data from CSV files and create a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71f1d227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>receipt_id</th>\n",
       "      <th>receipts_item_id</th>\n",
       "      <th>purchase_time</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>format</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>quantity</th>\n",
       "      <th>yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9622257</td>\n",
       "      <td>32369294</td>\n",
       "      <td>7/22/2018 21:19</td>\n",
       "      <td>Rice</td>\n",
       "      <td>Rice</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9446359</td>\n",
       "      <td>31885876</td>\n",
       "      <td>7/15/2018 16:17</td>\n",
       "      <td>Rice</td>\n",
       "      <td>Rice</td>\n",
       "      <td>minimarket</td>\n",
       "      <td>102750.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9470290</td>\n",
       "      <td>31930241</td>\n",
       "      <td>7/15/2018 12:12</td>\n",
       "      <td>Rice</td>\n",
       "      <td>Rice</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9643416</td>\n",
       "      <td>32418582</td>\n",
       "      <td>7/24/2018 8:27</td>\n",
       "      <td>Rice</td>\n",
       "      <td>Rice</td>\n",
       "      <td>minimarket</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9692093</td>\n",
       "      <td>32561236</td>\n",
       "      <td>7/26/2018 11:28</td>\n",
       "      <td>Rice</td>\n",
       "      <td>Rice</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>124500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  receipt_id  receipts_item_id    purchase_time category  \\\n",
       "0           1     9622257          32369294  7/22/2018 21:19     Rice   \n",
       "1           2     9446359          31885876  7/15/2018 16:17     Rice   \n",
       "2           3     9470290          31930241  7/15/2018 12:12     Rice   \n",
       "3           4     9643416          32418582   7/24/2018 8:27     Rice   \n",
       "4           5     9692093          32561236  7/26/2018 11:28     Rice   \n",
       "\n",
       "  sub_category       format  unit_price  discount  quantity yearmonth  \n",
       "0         Rice  supermarket    128000.0         0         1   2018-07  \n",
       "1         Rice   minimarket    102750.0         0         1   2018-07  \n",
       "2         Rice  supermarket     64000.0         0         3   2018-07  \n",
       "3         Rice   minimarket     65000.0         0         1   2018-07  \n",
       "4         Rice  supermarket    124500.0         0         1   2018-07  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv files\n",
    "\n",
    "rice = pd.read_csv('data_input/rice.csv')\n",
    "rice.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036aa76",
   "metadata": {},
   "source": [
    "Absolutely, reading different types of files using pandas is quite straightforward. The `read_csv()` function is a versatile tool, and pandas provides similar functions for reading other file formats as well. Let's explore a reference example for reading various file types:\n",
    "\n",
    "**1. Reading TXT Files:**\n",
    "For tab-separated values (TSV) in a TXT file named `data.txt`, you can use `read_csv()` and specify the delimiter as `\\t`:\n",
    "\n",
    "```python\n",
    "# Read tab-separated TXT file\n",
    "txt_file_path = 'data.txt'\n",
    "df_txt = pd.read_csv(txt_file_path, sep='\\t')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_txt.head())\n",
    "```\n",
    "\n",
    "**2. Reading Excel (XLSX) Files:**\n",
    "To read data from Excel files, such as `data.xlsx`, you can use `read_excel()`:\n",
    "\n",
    "```python\n",
    "# Read Excel file\n",
    "xlsx_file_path = 'data.xlsx'\n",
    "df_xlsx = pd.read_excel(xlsx_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_xlsx.head())\n",
    "```\n",
    "\n",
    "**3. Reading JSON Files:**\n",
    "JSON files can be read using `read_json()`. If you have a JSON file named `data.json`:\n",
    "\n",
    "```python\n",
    "# Read JSON file\n",
    "json_file_path = 'data.json'\n",
    "df_json = pd.read_json(json_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_json.head())\n",
    "```\n",
    "\n",
    "**4. Reading HTML Tables (from URL):**\n",
    "Pandas can even read HTML tables directly from URLs using `read_html()`:\n",
    "\n",
    "```python\n",
    "# Read HTML table from URL\n",
    "url = 'https://example.com/table.html'\n",
    "tables = pd.read_html(url)\n",
    "df_html = tables[0]  # Assuming the desired table is the first one\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_html.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_7_2_'></a>[Reading SQLite Databases](#toc0_)\n",
    "\n",
    "## <a id='toc2_8_'></a>[Database Connection](#toc0_)\n",
    "\n",
    "There are numerous Python packages that provide functionalities for data analysts to work with databases. Here are some examples:\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-success\">\n",
    "<details>\n",
    "    <summary><b>✨ Connecting to MySQL</b></summary>\n",
    "    \n",
    "```python\n",
    "import pymysql\n",
    "  \n",
    "conn = pymysql.connect(\n",
    "    host = HOST_NAME,\n",
    "    port = PORT_NUMBER,\n",
    "    user = USER_NAME,\n",
    "    password = PASSWORD,\n",
    "    db = DATABASE_NAME)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "    <summary><b>✨ Connecting to Oracle</b></summary>\n",
    "    \n",
    "```python\n",
    "import cx_Oracle\n",
    "  \n",
    "# data source name from tnsnames.ora file\n",
    "dsn_tns = cx_Oracle.makedsn(\n",
    "    HOST_NAME,\n",
    "    PORT_NUMBER\n",
    "    service_name = SERVICE_NAME)\n",
    "\n",
    "# connection\n",
    "conn = cx_Oracle.connect(\n",
    "    user = USER_NAME,\n",
    "    password = PASSWORD,\n",
    "    dsn = dsn_tns)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "    <summary><b>✨ Connecting to PostgreSQL</b></summary>\n",
    "    \n",
    "```python\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host = HOST_NAME,\n",
    "    port = PORT_NUMBER,\n",
    "    user = USER_NAME,\n",
    "    password = PASSWORD,\n",
    "    database = DATABASE_NAME)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "    <summary><b>✨ Connecting to Microsoft SQL Server</b></summary>\n",
    "    \n",
    "```python\n",
    "import pyodbc \n",
    "conn = pyodbc.connect(\n",
    "    'Driver={ODBC Driver 17 for SQL Server};'\n",
    "    'Server=host;'\n",
    "    'PORT=1433;'\n",
    "    'UID=user;'\n",
    "    'PWD=password;'\n",
    "    'Database=database;')\n",
    "```\n",
    "</details>\n",
    "</div> \n",
    "\n",
    "Then, to read the data, we use `pd.read_sql_query()` and provide the established connection:\n",
    "\n",
    "```python\n",
    "sales = pd.read_sql_query(\"SELECT * FROM sales\", conn)\n",
    "```\n",
    "\n",
    "When passing the `conn` object, `pandas` uses [SQLAlchemy](https://www.sqlalchemy.org/), making it compatible with various databases. Rest assured, this is not something you need to worry about in this learning stage. As an initial step, let's try connecting Jupyter Notebook to an SQLite database (using the `sqlite3` package) referred to as the **connection**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb1df44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "097f6f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Connection at 0x7fed8170d540>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('data_input/chinook.db')\n",
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f6961e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from SQL query into DataFrame\n",
    "query = 'SELECT * FROM albums'\n",
    "data = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f20dcb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlbumId</th>\n",
       "      <th>Title</th>\n",
       "      <th>ArtistId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>For Those About To Rock We Salute You</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Balls to the Wall</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Restless and Wild</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Let There Be Rock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Big Ones</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AlbumId                                  Title  ArtistId\n",
       "0        1  For Those About To Rock We Salute You         1\n",
       "1        2                      Balls to the Wall         2\n",
       "2        3                      Restless and Wild         2\n",
       "3        4                      Let There Be Rock         1\n",
       "4        5                               Big Ones         3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Summary](#toc0_)\n",
    "\n",
    "In conclusion, this section provided an introduction to Python environment. Additionally, we covered the basics of Python programming for language preprocessing, including variables, data types, operations, and control structures. We delved into the field of Natural Language Processing (NLP) and discussed word embeddings, major text libraries in Python (such as NLTK and spaCy), and the importance of text preprocessing and tokenization. Through demonstrations and examples, we gained practical insights into utilizing these libraries for simple text processing tasks. By building a solid foundation in these areas, participants will be well-equipped to delve further into the fascinating world of Generative AI and LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_dev",
   "language": "python",
   "name": "llm_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
