{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Conservational AI with Large Language Models for Business"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritma\n",
    "\n",
    "Algoritma Data Science School is an educational institution based in Indonesia that specializes in providing training and courses in the field of data science and analytics. It aims to equip individuals with the knowledge and skills needed to excel in the rapidly growing field of data science.\n",
    "\n",
    "This Jupyter Notebook has been specifically designed for Algoritma Data Science School, providing a platform for students to explore, analyze, and visualize data using Python and its associated data science libraries. Jupyter Notebook serves as an interactive workspace where students can demonstrate their understanding of data science concepts, showcase their skills, and present their findings in a structured and organized manner. Please note that this Jupyter Notebook is intended for personal or educational use only. Kindly refrain from reproducing or distributing this notebook without prior permission. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "- Large Language Model (LLM)\n",
    "    - Overview of Large Language Model & Transformer\n",
    "    - Introduction to populer LLM like GPT-3, GPT-2, and BERT\n",
    "    - Understanding Capabilities and limitation of LLM\n",
    "\n",
    "- Large Language Model Implementation\n",
    "    - Introduction to LangChain\n",
    "    - Setting API key and .env\n",
    "    - LangChain QuickStart\n",
    "\n",
    "- Build Question-Answering System\n",
    "    - Introduction to Question-Answering System\n",
    "    - Connecting datasource (database and text data) with LLM\n",
    "    - Basics of building Question-Answering System using LLM with database and text data\n",
    "    - Demonstration of using OpenAI and LangChain to build a Question-Answering System\n",
    "\n",
    "- Hugging Face\n",
    "    - Introduction to Text Generation and Hugging Face\n",
    "    - Setting API key and .env\n",
    "    - Applying HuggingFace's inference API to use LLM without OpenAI credits\n",
    "    - Integrating HuggingFace's Inference API into the previously built Question-Answering System\n",
    "    - Demonstration of using HuggingFace's Inference API to build a Question-Answering System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model (LLM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is a framework for developing applications powered by language models. The LangChain framework is designed around these principle : \n",
    "\n",
    "1. Data-aware: connect a language model to other sources of data\n",
    "\n",
    "2. Agentic: allow a language model to interact with its environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting API key and .env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dotenv\n",
    "\n",
    "The dotenv library is a popular Python library that simplifies the process of loading environment variables from a .env file into your Python application. It allows you to store configuration variables separately from your code, making it easier to manage sensitive information such as API keys, database credentials, or other environment-specific settings.\n",
    "\n",
    "#### `.env` file\n",
    "\n",
    "the .env file is a text file commonly used in software development projects to store environment-specific configuration variables. It follows a simple key-value format, where each line represents a single configuration variable.\n",
    "\n",
    "Here are a few important points about the .env file:\n",
    "\n",
    "1. Purpose: The primary purpose of the .env file is to separate sensitive or environment-specific information from your codebase. It allows you to store configuration variables such as API keys, database credentials, or other settings that may change based on the environment (e.g., development, staging, production).\n",
    "\n",
    "2. File Format: The .env file is typically a plain text file without any special formatting. Each line in the file consists of a key-value pair, where the key and value are separated by an equal sign (=). For example :\n",
    "\n",
    "```{python}\n",
    "API_KEY=abc123\n",
    "DATABASE_URL=mysql://user:password@localhost/db\n",
    "\n",
    "```\n",
    "\n",
    "3. Environment Variables: Each line in the .env file represents an environment variable. The key is the name of the environment variable, and the value is the corresponding value for that variable. These variables can be accessed within your code to retrieve the associated values.\n",
    "\n",
    "4. Loading Variables: To make use of the variables defined in the .env file, you need to load them into your application. This is typically done using a library like dotenv in Python. The library reads the .env file and sets the defined variables as environment variables that can be accessed within your code.\n",
    "\n",
    "5. Security: It's essential to ensure the security of your .env file. The file may contain sensitive information, such as passwords or access tokens. Make sure to exclude the .env file from version control systems like Git and only share it with authorized individuals who require access to the environment-specific configuration.\n",
    "\n",
    "Overall, the .env file provides a convenient and flexible way to manage configuration variables in your project, allowing you to keep sensitive information separate from your code and easily configure different environments.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain QuickStart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic**\n",
    "\n",
    "- Prompt\n",
    "- Chain\n",
    "- Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9) # parameter temperature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Prompt\n",
    "\n",
    "The basic building of LangChain is the LLM, which takes in text and generates more text (answer!).\n",
    "\n",
    "Example, we're building an application that generates a brand name based on company description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Uptown Burgers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a good name for a brand that makes local burger?\"\n",
    "\n",
    "print(llm.predict(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice every re-run it generate new answer.\n",
    "\n",
    "We can also did it in other languages, let's try with Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PangoMentai.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(\"Nama yang bagus untuk brand yang membuat pisang goreng mentai?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most LLM applications do not pass input directly into an LLM. Usually they will add the usre input to a larger piece of text, called a prompt template.\n",
    "\n",
    "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.\n",
    "\n",
    "The prompt template may contain:\n",
    "\n",
    "- instructions to the language model,\n",
    "\n",
    "- a set of few shot examples to help the language model generate a better response,\n",
    "\n",
    "- a question to the language model.\n",
    "\n",
    "In the previous example, the text we passed to the model contained instruction to generate a brand name based on description. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a brand that makes local burger?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_prompt = PromptTemplate.from_template(\"What is a good name for a brand that makes {product}?\")\n",
    "\n",
    "prompt = template_prompt.format(product=\"local burger\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the instruction changes automatically based on user input, this instruction will be input to `llm` to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BurgTowne.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a template, it can handle more than one input, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Write a {adjective} poem about {subject}\"\n",
    "\n",
    "poem_template = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"subject\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a sad poem about ducks'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_template.format(adjective='sad', subject='ducks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The water's so still, not a soul around\n",
      "The ducks paddle in circles, the sun setting down\n",
      "The lake is so empty it fills them with dread\n",
      "No one to talk to, no one to share bread\n",
      "\n",
      "The sky is now vacant and void of all sound\n",
      "No one to share warmth, no one around\n",
      "The ducks look in vain for their partners in flight\n",
      "But the sky has remained empty, no birds in sight\n",
      "\n",
      "The lonesome ducks feel their hearts break in two\n",
      "No one is near, what else can they do?\n",
      "No quacks or honks, no chatter or calls\n",
      "Just quiet and sadness and these empty walls\n",
      "\n",
      "The ducks will swim on, though their hearts may be sore\n",
      "Until one day, the sky will roar\n",
      "And their friends will return, with the sound of a bell\n",
      "Until then, the ducks will stay in their lonely and sad little hell.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(poem_template.format(adjective='sad', subject='ducks')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template may contain:\n",
    "\n",
    "- instructions to the language model,\n",
    "\n",
    "- a set of few shot examples to help the language model generate a better response,\n",
    "\n",
    "- a question to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BatikBliss\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "\n",
    "Here are some examples of good company names:\n",
    "\n",
    "- search engine, Google\n",
    "- social media, Facebook\n",
    "- video sharing, YouTube\n",
    "\n",
    "The name should be short, catchy and easy to remember.\n",
    "\n",
    "What is a good name for a brand that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "brand_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "batik_prompt = brand_template.format(product='batik')\n",
    "\n",
    "print(llm.predict(batik_prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "We've got a model and prompt template, we'll want to combine the two by \"Chain\"-ing them up. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.\n",
    "\n",
    "The simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.\n",
    "\n",
    "For example, if we want to generate response from our template our workflow would be:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the prompt based on input with `template_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a brand that makes rendang mozarella?\n"
     ]
    }
   ],
   "source": [
    "prompt = template_prompt.format(product=\"rendang mozarella\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate response from prompt with `llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mozarella Rendang Co.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify the workflow by chaining (link) them up with `Chains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mozarella Rendang Delights\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('rendang mozarella'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every new input we just need one line code to generate the response using `Chains`. Understanding how this simple chain works will set you up well for working with more complex chains."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a plan for our initial chain to follow specific steps. However, for more complicated workflows, it's important to be able to pick actions based on the situation.\n",
    "\n",
    "Agents help us do exactly that. They use a language model to figure out which actions to take and in what order. These agents have tools at their disposal, and they keep selecting a tool, running it, and examining the results until they find the ultimate solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load an agent, you need to choose a(n):\n",
    "\n",
    "- **LLM/Chat model:** The language model powering the agent.\n",
    "\n",
    "- **Tool(s):** A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the Tools documentation.\n",
    "\n",
    "- **Agent name:** A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see here. For a list of supported agents and their specifications, see here.\n",
    "\n",
    "For this example, we'll be using `wikipedia` tools to query a response from wikipedia information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# The language model we're going to use to control the agent.\n",
    "llm_agent = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm_agent)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm_agent, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Messi joined Barcelona and then calculate his current age raised to the 0.43 power.\n",
      "Action: Wikipedia\n",
      "Action Input: Lionel Messi\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Lionel Messi\n",
      "Summary: Lionel Andrés Messi (Spanish pronunciation: [ljoˈnel anˈdɾes ˈmesi] (listen); born 24 June 1987), also known as Leo Messi, is an Argentine professional footballer who plays as a forward and captains the Argentina national team. Widely regarded as one of the greatest players of all time, Messi has won a record seven Ballon d'Or awards and a record six European Golden Shoes, and in 2020 he was named to the Ballon d'Or Dream Team. Until leaving the club in 2021, he had spent his entire professional career with Barcelona, where he won a club-record 34 trophies, including ten La Liga titles, seven Copa del Rey titles and the UEFA Champions League four times. With his country, he won the 2021 Copa América and the 2022 FIFA World Cup. A prolific goalscorer and creative playmaker, Messi holds the records for most goals in La Liga (474), most hat-tricks in La Liga (36) and the UEFA Champions League (eight), and most assists in La Liga (192) and the Copa América (17). He also has the most international goals by a South American male (103). Messi has scored over 800 senior career goals for club and country, and has the most goals by a player for a single club (672).\n",
      "Messi relocated to Spain from Argentina aged 13 to join Barcelona, for whom he made his competitive debut aged 17 in October 2004. He established himself as an integral player for the club within the next three years, and in his first uninterrupted season in 2008–09 he helped Barcelona achieve the first treble in Spanish football; that year, aged 22, Messi won his first Ballon d'Or. Three successful seasons followed, with Messi winning four consecutive Ballons d'Or, making him the first player to win the award four times. During the 2011–12 season, he set the La Liga and European records for most goals scored in a single season, while establishing himself as Barcelona's all-time top scorer. The following two seasons, Messi finished second for the Ballon d'Or behind Cristiano Ronaldo (his perceived career rival), before regaining his best form during the 2014–15 campaign, becoming the all-time top scorer in La Liga and leading Barcelona to a historic second treble, after which he was awarded a fifth Ballon d'Or in 2015. Messi assumed captaincy of Barcelona in 2018, and won a record sixth Ballon d'Or in 2019. Out of contract, he signed for French club Paris Saint-Germain in August 2021, spending two seasons at the club and winning Ligue 1 twice.\n",
      "An Argentine international, Messi is the country's all-time leading goalscorer and also holds the national record for appearances. At youth level, he won the 2005 FIFA World Youth Championship, finishing the tournament with both the Golden Ball and Golden Shoe, and an Olympic gold medal at the 2008 Summer Olympics. His style of play as a diminutive, left-footed dribbler drew comparisons with his compatriot Diego Maradona, who described Messi as his successor. After his senior debut in August 2005, Messi became the youngest Argentine to play and score in a FIFA World Cup (2006), and reached the final of the 2007 Copa América, where he was named young player of the tournament. As the squad's captain from August 2011, he led Argentina to three consecutive finals: the 2014 FIFA World Cup, for which he won the Golden Ball, the 2015 Copa América, winning the Golden Ball, and the 2016 Copa América. After announcing his international retirement in 2016, he reversed his decision and led his country to qualification for the 2018 FIFA World Cup, a third-place finish at the 2019 Copa América, and victory in the 2021 Copa América, while winning the Golden Ball and Golden Boot for the latter. For this achievement, Messi received a record-extending seventh Ballon d'Or in 2021. In 2022, he led Argentina to win the 2022 FIFA World Cup, where he won a record second Golden Ball, scored seven goals including two in the final, and broke the record for most games played at the World Cup (26).\n",
      "Messi has endorsed sportswear company \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to calculate his current age raised to the 0.43 power.\n",
      "Action: Calculator\n",
      "Action Input: 24^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.9218486893172186\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Lionel Messi joined Barcelona in 2004 and his current age raised to the 0.43 power is 3.9218486893172186.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lionel Messi joined Barcelona in 2004 and his current age raised to the 0.43 power is 3.9218486893172186.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What year did Lionel Messi Joined Barcelona? What is his current age raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Question Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Question-Answering System\n",
    "- Introduction to Question-Answering System\n",
    "- Connecting datasource (database and text data) with LLM\n",
    "- Basics of building Question-Answering System using LLM with database and text data\n",
    "- Demonstration of using OpenAI and LangChain to build a Question-Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Question-Answer System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, LangChain is an open-source library that offers developers a comprehensive set of resources to develop applications that run on Large Language Models (LLMs). In the previous example, we have built model that feed question and llm will generate response based on its internal knowledge.\n",
    "\n",
    "But, what if we need to feed a question that more contextual about our domain business problem? for example what if we feed LLM with question about our company such as \"What is the product that generate most revenue?\".\n",
    "However, there are some limited knowledge of LLMs models, especially when documents are specific of some context.\n",
    "\n",
    "One way to address this limitation is to give more information about documents to large language model (LLM) to answer questions about information it was not trained on. The basic idea is to first retrieve any relevant documents from a corpus called context, then pass those documents along with the original question to the LLM. The LLM will then generate a response that is informed by the information in the retrieved documents.\n",
    "\n",
    "This documents can be any file that store any information. It can be database, pdf, text and even information from a website.\n",
    "\n",
    "In this module, we will explore how to connect and feed a database and text information to LLM to build Question-Answer System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provide function that connect database to LLM, it called SQL Database. It also provide a function to chaining between the database, model llm and an agent that will execute SQL query based on natural language prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase, SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part we need to load the data, we will use the chinook data from our academy class as example. You need to explicitly explain what kind of database you load, for example `sqlite:///`.\n",
    "\n",
    "Then you can just load the database using `SQLDatabase` from `langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dburi = \"sqlite:///data_input/chinook.db\"\n",
    "db = SQLDatabase.from_uri(dburi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we chaining the `db` to model it will create agent that will looking for answer in chinook database based on prompt input.\n",
    "\n",
    "Let's try prompt to know how many rows in the tracks table of this db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0) # parameter temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain_test\\lib\\site-packages\\langchain\\chains\\sql_database\\base.py:63: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "How many rows is in the tracks table of this db?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) AS 'Number of Tracks' FROM tracks;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(3503,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m3503\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3503'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "\n",
    "db_chain.run(\"How many rows is in the tracks table of this db?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are some component in the output, such as `SQLQuery` that provide information what process the model did to seeking the answer using SQL; `SQLResult`, the result of `SQLQuery` from our database; And lastly, it convert the `SQLResult` to natural language and display it on `Answer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Building Question-Answer System using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SqlDatabaseChain` allows you to answer questions over a SQL database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example we use the question from our dive deeper. \n",
    "\n",
    "> all sales in rock genre in 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "all sales in rock genre in 2012 based on invoice\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"Total\" \n",
      "FROM invoices \n",
      "INNER JOIN invoice_items \n",
      "ON invoices.\"InvoiceId\" = invoice_items.\"InvoiceId\" \n",
      "INNER JOIN tracks \n",
      "ON invoice_items.\"TrackId\" = tracks.\"TrackId\" \n",
      "INNER JOIN genres \n",
      "ON tracks.\"GenreId\" = genres.\"GenreId\" \n",
      "WHERE genres.\"Name\" = \"Rock\" \n",
      "AND invoices.\"InvoiceDate\" >= date('2012-01-01') \n",
      "AND invoices.\"InvoiceDate\" <= date('2012-12-31') \n",
      "LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(8.91,), (8.91,), (8.91,), (8.91,), (13.86,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe total sales of rock genre in 2012 based on invoice is 8.91, 8.91, 8.91, 8.91, 13.86.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The total sales of rock genre in 2012 based on invoice is 8.91, 8.91, 8.91, 8.91, 13.86.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"all sales in rock genre in 2012 based on invoice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We want the returned DataFrame to contain only the Pop genre and only when the UnitPrice of the track is 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "We want the returned DataFrame to contain only the Pop genre and only when the UnitPrice of the track is 0.99\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT TrackId, Name, GenreId, UnitPrice FROM tracks WHERE GenreId=3 AND UnitPrice=0.99 LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(77, 'Enter Sandman', 3, 0.99), (78, 'Master Of Puppets', 3, 0.99), (79, 'Harvester Of Sorrow', 3, 0.99), (80, 'The Unforgiven', 3, 0.99), (81, 'Sad But True', 3, 0.99)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe returned DataFrame contains only the Pop genre and only when the UnitPrice of the track is 0.99.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The returned DataFrame contains only the Pop genre and only when the UnitPrice of the track is 0.99.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"We want the returned DataFrame to contain only the Pop genre and only when the UnitPrice of the track is 0.99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Tampilkan lagu dengan Genre Pop\n",
      "SQLQuery:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mSELECT \"Name\" FROM tracks WHERE \"GenreId\"=3 LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Enter Sandman',), ('Master Of Puppets',), ('Harvester Of Sorrow',), ('The Unforgiven',), ('Sad But True',)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mEnter Sandman, Master Of Puppets, Harvester Of Sorrow, The Unforgiven, Sad But True\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Enter Sandman, Master Of Puppets, Harvester Of Sorrow, The Unforgiven, Sad But True'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"Tampilkan lagu dengan Genre Pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes: In each query there is always `LIMIT 5` query command, this is the limitation from the model (at least we got the query tho and run it by ourselves to get the full rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Dataset (CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured data not only stored in database file, some other example are .xlsx and .csv that stored data as table (columns and rows). Likewise, LangChain not only provides agent to generate answer from database using SQL based on natural language prompt, it also provides agent to generate answer based on tabular text data source, such as csv, that we will demonstrate how to utilize the agent.\n",
    "\n",
    "First, let's define the location path of our dataset `rice.csv` that contains rice category transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data_input/rice.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create agent. This time is a CSV agent. We use same llm model as our sql part that's why we don't need to re-define llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_csv_agent\n",
    "agent = create_csv_agent(llm, filepath, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just run ask the question about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: saya harus mencari total pembelian dalam setiap format\n",
      "Action: python_repl_ast\n",
      "Action Input: df.groupby('format')['quantity'].sum()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mformat\n",
      "hypermarket    1464\n",
      "minimarket     9578\n",
      "supermarket    4953\n",
      "Name: quantity, dtype: int64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya sekarang tahu jawabannya\n",
      "Final Answer: Hypermarket: 1464; Minimarket: 9578; Supermarket: 4953\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hypermarket: 1464; Minimarket: 9578; Supermarket: 4953'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"berikan detail banyaknya transaksi yang terjadi di setiap format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there more component in the output.\n",
    "- `Thought`, is what agent thought how to solve the problem from the prompt and what is thought based on its action result.\n",
    "- `Action`, is what agent did to solve the problem, it use `python_repl_ast` which is just python shell, it tells the agent use python to solved the problem. It also tell what `pandas` command the agent did to get the result from csv data.\n",
    "- `Final Answer`: is a natural language form of answer from the result of `Action Input`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of using OPENAI and Langchain to build a Question-Answering System For Unstrutured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document in the company not only stored in structured form of data, it also stored in unstructured form of data. For example like summary of the meeting, task reports, product description, etc. If we need to gather information or ask a question regarding those documents, a person would looking related document and search for the answer manually.\n",
    "\n",
    "So what if we want to let llm model to seeking the answer? What if we have a document or regulation that only special at our company. We can add our own information to the LLM model. This part we will use OpenAI Embeddings. So we can build question answer system based on our company document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `summary.txt`` that contains summary of coal news for Australia, Indonesia, and China."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('data_input/summary.txt')\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "#texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Australia's coal and gas exports may reduce by half within the next five years due to the efforts of Asian countries to decrease greenhouse gas emissions. The earnings of minerals and energy exports are predicted to reach $464bn in 2022-23 from $128bn in thermal coal exports and $91bn in liquidified natural gas (LNG) exports. Coal producers are in talks with the government of New South Wales, following the government's announcement that coal miners should reserve up to 10% of production for domestic supply to control rising energy costs in Australia. Exports of coal are essential to the Australian economy, with 80% of the country's coal exported, yet the move comes as coal prices rise nearly 50% YoY.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(\"What are the effects of legislations surrounding emissions on the Australia coal market?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, Indonesia has imposed a ban on coal exports. This is to ensure adequate supply for the country's state-owned electricity companies.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(\"Is there an export ban on Coal in Indonesia? Why?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The main exporters of coal to China are Indonesia, Russia, and Mongolia. Indonesia is the largest exporter of coal to China, accounting for 58.3% of total imports, followed by Russia at 23.3% and Mongolia at 10%.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(\"Who are the main exporters of Coal to China? What is the role of Indonesia in this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using other LLM Model\n",
    "\n",
    "### Hugging Face\n",
    "\n",
    "Hugging Face is a company that specializes in natural language processing (NLP) and develops a variety of tools, libraries, and models to facilitate NLP tasks. The company is known for creating and maintaining an open-source library called \"transformers,\" which has gained significant popularity in the NLP community.\n",
    "\n",
    "The Transformers library, developed by Hugging Face, provides a wide range of pre-trained models and utilities for tasks such as text classification, named entity recognition, sentiment analysis, machine translation, question answering, and more. These pre-trained models are based on state-of-the-art architectures, such as the GPT (Generative Pre-trained Transformer) models developed by OpenAI.\n",
    "\n",
    "In addition to the Transformers library, Hugging Face offers a platform called the \"Hugging Face Hub.\" This platform serves as a central repository where users can access, share, and collaborate on models and datasets. It provides a seamless integration with the Transformers library, allowing users to easily download and use pre-trained models.\n",
    "\n",
    "Hugging Face has made significant contributions to the NLP community by democratizing access to pre-trained models and fostering collaboration among researchers and developers. Their tools and resources have greatly simplified the process of building NLP applications and have contributed to the rapid advancement of the field.\n",
    "\n",
    "Langchain also provide us to connect with hugging face API so we can use another Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar with the OpenAI part the only diffrent we need to define which model we want to use, then we set the model parameter such as temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain_test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id='gpt2',\n",
    "    model_kwargs={'temperature': 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name, we need to create a chain, so we need to provide some kind on prompt template to the model. Then we create the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "hub_chain = LLMChain(prompt=prompt, llm=hub_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask some intereting question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: who won FIFA World Cup in the year 1994?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nA: The winner of the'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_chain.run(\"who won FIFA World Cup in the year 1994?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating HuggingFace's Inference API into the previously built Question-Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase, SQLDatabaseChain\n",
    "\n",
    "dburi = \"sqlite:///data_input/chinook.db\"\n",
    "db = SQLDatabase.from_uri(dburi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to replace the llm model with the hugging face model when chaining it to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm_t5 = HuggingFaceHub(\n",
    "    repo_id='mrm8488/t5-base-finetuned-wikiSQL',\n",
    "    model_kwargs={'temperature': 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_db = PromptTemplate(\n",
    "    input_variables=['question'],\n",
    "    template=\"Translate English to SQL: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate English to SQL: How many rows is in the tracks's table?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT Rows FROM table WHERE Tracks = Tracks'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_chain = LLMChain(prompt=prompt_db, llm=hf_llm_t5, verbose=True)\n",
    "\n",
    "hub_chain.run(\"How many rows is in the tracks's table?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't provide correct answer because it should be FROM tracks instead of table. Notice this is the difference between t5 model and openai model.\n",
    "\n",
    "However, HuggingFace provide many models that can use for certain task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While GPT-2 is a powerful language model with impressive capabilities, it does have certain weaknesses:\n",
    "\n",
    "- Lack of Factual Accuracy: GPT-2 generates text based on patterns and examples it has learned from training data, but it does not have a built-in mechanism to verify the accuracy of the information it generates. As a result, it may sometimes produce plausible-sounding but factually incorrect or misleading responses.\n",
    "\n",
    "- Limited Context Understanding: GPT-2's understanding of context is limited to a fixed window of text. It does not possess true contextual understanding or long-term memory. This can lead to issues where it fails to maintain coherence or consistency when generating longer passages of text.\n",
    "\n",
    "- Sensitivity to Input Phrasing: GPT-2 can be sensitive to slight changes in input phrasing, which can sometimes result in different or inconsistent responses. Small modifications in the wording of a prompt can lead to significantly different outputs, making it challenging to control the model's behavior consistently.\n",
    "\n",
    "- Lack of Explainability: GPT-2 operates as a black box model, meaning it does not provide explanations for the reasoning behind its generated responses. This lack of transparency can make it difficult to understand how and why the model arrives at certain conclusions or generates specific outputs, making it less suitable for applications where explainability is critical.\n",
    "\n",
    "- Propensity for Biased Outputs: GPT-2 is trained on vast amounts of text data from the internet, which can contain biases and reflect societal prejudices. As a result, the model may inadvertently generate biased or prejudiced responses, perpetuating or amplifying existing biases present in the training data.\n",
    "\n",
    "- Vulnerability to Adversarial Inputs: GPT-2 can be susceptible to generating misleading or nonsensical outputs when presented with adversarial inputs or deliberate attempts to manipulate its behavior. Adversarial examples can exploit weaknesses in the model's training data or architecture, leading to unreliable or undesirable responses.\n",
    "\n",
    "It's important to note that some of these weaknesses have been addressed or mitigated in subsequent models like GPT-3 and GPT-4, which have shown improvements in contextual understanding, bias handling, and fine-grained control."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_test",
   "language": "python",
   "name": "langchain_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
