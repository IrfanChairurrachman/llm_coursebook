{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Conservational AI with Large Language Models for Business"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritma\n",
    "\n",
    "Algoritma Data Science School is an educational institution based in Indonesia that specializes in providing training and courses in the field of data science and analytics. It aims to equip individuals with the knowledge and skills needed to excel in the rapidly growing field of data science.\n",
    "\n",
    "This Jupyter Notebook has been specifically designed for Algoritma Data Science School, providing a platform for students to explore, analyze, and visualize data using Python and its associated data science libraries. Jupyter Notebook serves as an interactive workspace where students can demonstrate their understanding of data science concepts, showcase their skills, and present their findings in a structured and organized manner. Please note that this Jupyter Notebook is intended for personal or educational use only. Kindly refrain from reproducing or distributing this notebook without prior permission. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "- Large Language Model (LLM)\n",
    "    - Overview of Large Language Model & Transformer\n",
    "    - Introduction to populer LLM like GPT-3, GPT-2, and BERT\n",
    "    - Understanding Capabilities and limitation of LLM\n",
    "\n",
    "- Large Language Model Implementation\n",
    "    - Introduction to LangChain\n",
    "    - Setting API key and .env\n",
    "    - LangChain QuickStart\n",
    "\n",
    "- Build Question-Answering System\n",
    "    - Introduction to Question-Answering System\n",
    "    - Connecting datasource (database and text data) with LLM\n",
    "    - Basics of building Question-Answering System using LLM with database and text data\n",
    "    - Demonstration of using OpenAI and LangChain to build a Question-Answering System\n",
    "\n",
    "- Hugging Face\n",
    "    - Introduction to Text Generation and Hugging Face\n",
    "    - Setting API key and .env\n",
    "    - Applying HuggingFace's inference API to use LLM without OpenAI credits\n",
    "    - Integrating HuggingFace's Inference API into the previously built Question-Answering System\n",
    "    - Demonstration of using HuggingFace's Inference API to build a Question-Answering System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model (LLM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is a framework for developing applications powered by language models. The LangChain framework is designed around these principle : \n",
    "\n",
    "1. Data-aware: connect a language model to other sources of data\n",
    "\n",
    "2. Agentic: allow a language model to interact with its environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting API key and .env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dotenv\n",
    "\n",
    "The dotenv library is a popular Python library that simplifies the process of loading environment variables from a .env file into your Python application. It allows you to store configuration variables separately from your code, making it easier to manage sensitive information such as API keys, database credentials, or other environment-specific settings.\n",
    "\n",
    "#### `.env` file\n",
    "\n",
    "the .env file is a text file commonly used in software development projects to store environment-specific configuration variables. It follows a simple key-value format, where each line represents a single configuration variable.\n",
    "\n",
    "Here are a few important points about the .env file:\n",
    "\n",
    "1. Purpose: The primary purpose of the .env file is to separate sensitive or environment-specific information from your codebase. It allows you to store configuration variables such as API keys, database credentials, or other settings that may change based on the environment (e.g., development, staging, production).\n",
    "\n",
    "2. File Format: The .env file is typically a plain text file without any special formatting. Each line in the file consists of a key-value pair, where the key and value are separated by an equal sign (=). For example :\n",
    "\n",
    "```{python}\n",
    "API_KEY=abc123\n",
    "DATABASE_URL=mysql://user:password@localhost/db\n",
    "\n",
    "```\n",
    "\n",
    "3. Environment Variables: Each line in the .env file represents an environment variable. The key is the name of the environment variable, and the value is the corresponding value for that variable. These variables can be accessed within your code to retrieve the associated values.\n",
    "\n",
    "4. Loading Variables: To make use of the variables defined in the .env file, you need to load them into your application. This is typically done using a library like dotenv in Python. The library reads the .env file and sets the defined variables as environment variables that can be accessed within your code.\n",
    "\n",
    "5. Security: It's essential to ensure the security of your .env file. The file may contain sensitive information, such as passwords or access tokens. Make sure to exclude the .env file from version control systems like Git and only share it with authorized individuals who require access to the environment-specific configuration.\n",
    "\n",
    "Overall, the .env file provides a convenient and flexible way to manage configuration variables in your project, allowing you to keep sensitive information separate from your code and easily configure different environments.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain QuickStart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic**\n",
    "\n",
    "- Prompt\n",
    "- Chain\n",
    "- Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9) # parameter temperature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Prompt\n",
    "\n",
    "The basic building of LangChain is the LLM, which takes in text and generates more text (answer!).\n",
    "\n",
    "Example, we're building an application that generates a brand name based on company description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "HomeTown Burgers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a good name for a brand that makes local burger?\"\n",
    "\n",
    "print(llm.predict(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice every re-run it generate new answer.\n",
    "\n",
    "We can also did it in other languages, let's try with Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MentaiFrits\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(\"Nama yang bagus untuk brand yang membuat pisang goreng mentai?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most LLM applications do not pass input directly into an LLM. Usually they will add the usre input to a larger piece of text, called a prompt template.\n",
    "\n",
    "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.\n",
    "\n",
    "The prompt template may contain:\n",
    "\n",
    "- instructions to the language model,\n",
    "\n",
    "- a set of few shot examples to help the language model generate a better response,\n",
    "\n",
    "- a question to the language model.\n",
    "\n",
    "In the previous example, the text we passed to the model contained instruction to generate a brand name based on description. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a brand that makes local burger?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_prompt = PromptTemplate.from_template(\"What is a good name for a brand that makes {product}?\")\n",
    "\n",
    "prompt = template_prompt.format(product=\"local burger\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the instruction changes automatically based on user input, this instruction will be input to `llm` to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Burgers by the Bay.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a template, it can handle more than one input, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Write a {adjective} poem about {subject}\"\n",
    "\n",
    "poem_template = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"subject\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a sad poem about ducks'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_template.format(adjective='sad', subject='ducks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Oh, little ducks in sorrow\n",
      "Sad are your cries\n",
      "Your little feet leave a sorrowful trail\n",
      "On the mirror lakes you glide\n",
      "\n",
      "You quack so sadly in pain\n",
      "Your sorrowful quacks echoing\n",
      "A mournful reminder of your sadness\n",
      "And a call to understand\n",
      "\n",
      "Oh, little ducks I do not know\n",
      "The pain of your sorrow\n",
      "But I would like to share your load\n",
      "To carry it with you tomorrow\n",
      "\n",
      "For little ducks are so sad\n",
      "Their little hearts so strong\n",
      "They may not understand the pain\n",
      "But they still keep going on\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(poem_template.format(adjective='sad', subject='ducks')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template may contain:\n",
    "\n",
    "- instructions to the language model,\n",
    "\n",
    "- a set of few shot examples to help the language model generate a better response,\n",
    "\n",
    "- a question to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BatikBye.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "\n",
    "Here are some examples of good company names:\n",
    "\n",
    "- search engine, Google\n",
    "- social media, Facebook\n",
    "- video sharing, YouTube\n",
    "\n",
    "The name should be short, catchy and easy to remember.\n",
    "\n",
    "What is a good name for a brand that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "brand_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "batik_prompt = brand_template.format(product='batik')\n",
    "\n",
    "print(llm.predict(batik_prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "We've got a model and prompt template, we'll want to combine the two by \"Chain\"-ing them up. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.\n",
    "\n",
    "The simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.\n",
    "\n",
    "For example, if we want to generate response from our template our workflow would be:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the prompt based on input with `template_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a brand that makes rendang mozarella?\n"
     ]
    }
   ],
   "source": [
    "prompt = template_prompt.format(product=\"rendang mozarella\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate response from prompt with `llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rendez Mozzarella.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify the workflow by chaining (link) them up with `Chains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mozarella Rendang Co.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('rendang mozarella'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every new input we just need one line code to generate the response using `Chains`. Understanding how this simple chain works will set you up well for working with more complex chains."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a plan for our initial chain to follow specific steps. However, for more complicated workflows, it's important to be able to pick actions based on the situation.\n",
    "\n",
    "Agents help us do exactly that. They use a language model to figure out which actions to take and in what order. These agents have tools at their disposal, and they keep selecting a tool, running it, and examining the results until they find the ultimate solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load an agent, you need to choose a(n):\n",
    "\n",
    "- **LLM/Chat model:** The language model powering the agent.\n",
    "\n",
    "- **Tool(s):** A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the Tools documentation.\n",
    "\n",
    "- **Agent name:** A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see here. For a list of supported agents and their specifications, see here.\n",
    "\n",
    "For this example, we'll be using `wikipedia` tools to query a response from wikipedia information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# The language model we're going to use to control the agent.\n",
    "llm_agent = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm_agent)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm_agent, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Messi joined Barcelona and then calculate his current age raised to the 0.43 power.\n",
      "Action: Wikipedia\n",
      "Action Input: Lionel Messi\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Lionel Messi\n",
      "Summary: Lionel Andrés Messi (Spanish pronunciation: [ljoˈnel anˈdɾes ˈmesi] (listen); born 24 June 1987), also known as Leo Messi, is an Argentine professional footballer who plays as a forward for and captains the Argentina national team. Widely regarded as one of the greatest players of all time, Messi has won a record seven Ballon d'Or awards and a record six European Golden Shoes, and in 2020 he was named to the Ballon d'Or Dream Team. Until leaving the club in 2021, he had spent his entire professional career with Barcelona, where he won a club-record 34 trophies, including ten La Liga titles, seven Copa del Rey titles and the UEFA Champions League four times. With his country, he won the 2021 Copa América and the 2022 FIFA World Cup. A prolific goalscorer and creative playmaker, Messi holds the records for most goals in La Liga (474), most hat-tricks in La Liga (36) and the UEFA Champions League (eight), and most assists in La Liga (192) and the Copa América (17). He also has the most international goals by a South American male (103). Messi has scored over 800 senior career goals for club and country, and has the most goals by a player for a single club (672).\n",
      "Messi relocated to Spain from Argentina aged 13 to join Barcelona, for whom he made his competitive debut aged 17 in October 2004. He established himself as an integral player for the club within the next three years, and in his first uninterrupted season in 2008–09 he helped Barcelona achieve the first treble in Spanish football; that year, aged 22, Messi won his first Ballon d'Or. Three successful seasons followed, with Messi winning four consecutive Ballons d'Or, making him the first player to win the award four times. During the 2011–12 season, he set the La Liga and European records for most goals scored in a single season, while establishing himself as Barcelona's all-time top scorer. The following two seasons, Messi finished second for the Ballon d'Or behind Cristiano Ronaldo (his perceived career rival), before regaining his best form during the 2014–15 campaign, becoming the all-time top scorer in La Liga and leading Barcelona to a historic second treble, after which he was awarded a fifth Ballon d'Or in 2015. Messi assumed captaincy of Barcelona in 2018, and won a record sixth Ballon d'Or in 2019. Out of contract, he signed for French club Paris Saint-Germain in August 2021, spending two seasons at the club and winning Ligue 1 twice.\n",
      "An Argentine international, Messi is the country's all-time leading goalscorer and also holds the national record for appearances. At youth level, he won the 2005 FIFA World Youth Championship, finishing the tournament with both the Golden Ball and Golden Shoe, and an Olympic gold medal at the 2008 Summer Olympics. His style of play as a diminutive, left-footed dribbler drew comparisons with his compatriot Diego Maradona, who described Messi as his successor. After his senior debut in August 2005, Messi became the youngest Argentine to play and score in a FIFA World Cup (2006), and reached the final of the 2007 Copa América, where he was named young player of the tournament. As the squad's captain from August 2011, he led Argentina to three consecutive finals: the 2014 FIFA World Cup, for which he won the Golden Ball, the 2015 Copa América, winning the Golden Ball, and the 2016 Copa América. After announcing his international retirement in 2016, he reversed his decision and led his country to qualification for the 2018 FIFA World Cup, a third-place finish at the 2019 Copa América, and victory in the 2021 Copa América, while winning the Golden Ball and Golden Boot for the latter. For this achievement, Messi received a record-extending seventh Ballon d'Or in 2021. In 2022, he led Argentina to win the 2022 FIFA World Cup, where he won a record second Golden Ball, scored seven goals including two in the final, and broke the record for most games played at the World Cup (26).\n",
      "Messi has endorsed sportswear comp\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to calculate his current age raised to the 0.43 power.\n",
      "Action: Calculator\n",
      "Action Input: 24^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.9218486893172186\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Lionel Messi joined Barcelona in 2004 and his current age raised to the 0.43 power is 3.92.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lionel Messi joined Barcelona in 2004 and his current age raised to the 0.43 power is 3.92.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What year did Lionel Messi Joined Barcelona? What is his current age raised to the 0.43 power?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_test",
   "language": "python",
   "name": "langchain_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
